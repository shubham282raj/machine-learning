{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "from tensorflow.python.ops.resource_variable_ops import ResourceVariable\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking TensorFlow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Optimization with GradientTape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "Since TensorFlow Datasets are generators, you can't access directly the contents unless you iterate over them in a for loop, or by explicitly creating a Python iterator using `iter` and consuming its\n",
    "elements using `next`. Also, you can inspect the `shape` and `dtype` of each element using the `element_spec` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
    "test_dataset = h5py.File('datasets/test_signs.h5', \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_x'])\n",
    "y_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_y'])\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_x'])\n",
    "y_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(x_train)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing all unique labels\n",
    "unique_labels = set()\n",
    "for element in y_train:\n",
    "    unique_labels.add(element.numpy())\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_iter = iter(x_train)\n",
    "labels_iter = iter(y_train)\n",
    "plt.figure(figsize=(7, 7))\n",
    "for i in range(25):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(next(images_iter).numpy().astype(\"uint8\"))\n",
    "    plt.title(next(labels_iter).numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one more additional difference between TensorFlow datasets and Numpy arrays: If you need to transform one, you would invoke the `map` method to apply the function passed as an argument to each of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    image = tf.reshape(image, [-1,])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = x_train.map(normalize)\n",
    "new_test = x_test.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(new_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"new_train shape: {next(iter(new_train)).shape}\")\n",
    "print(f\"new_test shape: {next(iter(new_test)).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Function\n",
    "$Y = WX + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(np.random.randn(3,1), name=\"X\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function():\n",
    "    \"\"\"\n",
    "    Implement a linear function\n",
    "        Initialize X, W and b\n",
    "        X -> (3,1)\n",
    "        W -> (4,3)\n",
    "        b -> (4,1)\n",
    "    Returns:\n",
    "    result -- Y = WX + b\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    X = tf.constant(np.random.randn(3,1), name=\"X\")\n",
    "    W = tf.constant(np.random.randn(4,3), name=\"W\")\n",
    "    b = tf.constant(np.random.randn(4,1), name=\"b\")\n",
    "\n",
    "    Y = tf.add(tf.matmul(W, X) , b)\n",
    "\n",
    "    # Y = (W.matmul(X)).add(b) -- doesn't work in tensorflow\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = linear_function()\n",
    "print(result)\n",
    "\n",
    "assert type(result) == EagerTensor, \"Use the TensorFlow API\"\n",
    "assert np.allclose(result, [[-2.15657382], [ 2.95891446], [-1.08926781], [-0.84538042]]), \"Error\"\n",
    "print(\"\\033[92mAll test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    Z = tf.cast(Z, tf.float32)\n",
    "    A = tf.keras.activations.sigmoid(Z)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sigmoid(-1)\n",
    "print (\"type: \" + str(type(result)))\n",
    "print (\"dtype: \" + str(result.dtype))\n",
    "print (\"sigmoid(-1) = \" + str(result))\n",
    "print (\"sigmoid(0) = \" + str(sigmoid(0.0)))\n",
    "print (\"sigmoid(12) = \" + str(sigmoid(12)))\n",
    "\n",
    "def sigmoid_test(target):\n",
    "    result = target(0)\n",
    "    assert(type(result) == EagerTensor)\n",
    "    assert (result.dtype == tf.float32)\n",
    "    assert sigmoid(0) == 0.5, \"Error\"\n",
    "    assert sigmoid(-1) == 0.26894143, \"Error\"\n",
    "    assert sigmoid(12) == 0.99999386, \"Error\"\n",
    "\n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using One Hot Encodings\n",
    "define function for one label and then use map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(label, depth=6):\n",
    "    one_hot = tf.reshape(tf.one_hot(label, depth, axis=0), (depth,))\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix_test(target):\n",
    "    label = tf.constant(1)\n",
    "    depth = 4\n",
    "    result = target(label, depth)\n",
    "    print(\"Test 1:\",result)\n",
    "    assert result.shape[0] == depth, \"Use the parameter depth\"\n",
    "    assert np.allclose(result, [0., 1. ,0., 0.] ), \"Wrong output. Use tf.one_hot\"\n",
    "    label_2 = [2]\n",
    "    result = target(label_2, depth)\n",
    "    print(\"Test 2:\", result)\n",
    "    assert result.shape[0] == depth, \"Use the parameter depth\"\n",
    "    assert np.allclose(result, [0., 0. ,1., 0.] ), \"Wrong output. Use tf.reshape as instructed\"\n",
    "    \n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "one_hot_matrix_test(one_hot_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_test = y_test.map(one_hot_matrix)\n",
    "new_y_train = y_train.map(one_hot_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(new_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters\n",
    "Now you'll initialize a vector of numbers with the Glorot initializer. The function you'll be calling is `tf.keras.initializers.GlorotNormal`, which draws samples from a truncated normal distribution centered on 0, with `stddev = sqrt(2 / (fan_in + fan_out))`, where `fan_in` is the number of input units and `fan_out` is the number of output units, both in the weight tensor. \n",
    "\n",
    "To initialize with zeros or ones you could use `tf.zeros()` or `tf.ones()` instead. \n",
    "\n",
    " - `tf.keras.initializers.GlorotNormal(seed=1)`\n",
    " - `tf.Variable(initializer(shape=())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed = 1)\n",
    "\n",
    "    parameters = {}\n",
    "\n",
    "    parameters[\"W1\"] = tf.Variable(initializer(shape=(25, 12288)))\n",
    "    parameters[\"b1\"] = tf.Variable(initializer(shape=(25, 1)))\n",
    "    parameters[\"W2\"] = tf.Variable(initializer(shape=(12, 25)))\n",
    "    parameters[\"b2\"] = tf.Variable(initializer(shape=(12, 1)))\n",
    "    parameters[\"W3\"] = tf.Variable(initializer(shape=(6, 12)))\n",
    "    parameters[\"b3\"] = tf.Variable(initializer(shape=(6, 1)))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_test(target):\n",
    "    parameters = target()\n",
    "\n",
    "    values = {\"W1\": (25, 12288),\n",
    "              \"b1\": (25, 1),\n",
    "              \"W2\": (12, 25),\n",
    "              \"b2\": (12, 1),\n",
    "              \"W3\": (6, 12),\n",
    "              \"b3\": (6, 1)}\n",
    "\n",
    "    for key in parameters:\n",
    "        print(f\"{key} shape: {tuple(parameters[key].shape)}\")\n",
    "        assert type(parameters[key]) == ResourceVariable, \"All parameter must be created using tf.Variable\"\n",
    "        assert tuple(parameters[key].shape) == values[key], f\"{key}: wrong shape\"\n",
    "        assert np.abs(np.mean(parameters[key].numpy())) < 0.5,  f\"{key}: Use the GlorotNormal initializer\"\n",
    "        assert np.std(parameters[key].numpy()) > 0 and np.std(parameters[key].numpy()) < 1, f\"{key}: Use the GlorotNormal initializer\"\n",
    "\n",
    "    print(\"\\033[92mAll test passed\")\n",
    "    \n",
    "initialize_parameters_test(initialize_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Network in TensorFlow\n",
    "- Implement forward propagation\n",
    "- Retrieve the gradients and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "\n",
    "    Z1 = tf.add(tf.matmul(parameters[\"W1\"], X), parameters[\"b1\"])\n",
    "    A1 = tf.keras.activations.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(parameters[\"W2\"], A1), parameters[\"b2\"])\n",
    "    A2 = tf.keras.activations.relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(parameters[\"W3\"], A2), parameters[\"b3\"])\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_test(target, examples):\n",
    "    minibatches = examples.batch(2)\n",
    "    for minibatch in minibatches:\n",
    "        forward_pass = target(tf.transpose(minibatch), parameters)\n",
    "        print(forward_pass)\n",
    "        assert type(forward_pass) == EagerTensor, \"Your output is not a tensor\"\n",
    "        assert forward_pass.shape == (6, 2), \"Last layer must use W3 and b3\"\n",
    "        assert np.allclose(forward_pass, \n",
    "                            [[-0.13430887,  0.14086473],\n",
    "                             [ 0.21588647, -0.02582335],\n",
    "                             [ 0.7059658,   0.6484556 ],\n",
    "                             [-1.1260961,  -0.9329492 ],\n",
    "                             [-0.20181894, -0.3382722 ],\n",
    "                             [ 0.9558965,   0.94167566]]), \"Output does not match\"\n",
    "        break\n",
    "    \n",
    "\n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "forward_propagation_test(forward_propagation, new_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost\n",
    "```\n",
    "tf.keras.metrics.categorical_crossentropy(\n",
    "    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1\n",
    ")\n",
    "```\n",
    "Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    logits -- shape (6, num_examples)\n",
    "    labels -- \"true\" labels vector, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "\n",
    "    labels = tf.transpose(labels) # shape -> (num_examples, 6)\n",
    "    logits = tf.transpose(logits) # shape -> (num_examples, 6)\n",
    "\n",
    "    cost = tf.keras.losses.categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    # from_logits = False; \n",
    "    # By Defaults assumes y_pred encodes a probability distribution\n",
    "    cost = tf.reduce_mean(cost)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_test(target, Y):\n",
    "    pred = tf.constant([[ 2.4048107,   5.0334096 ],\n",
    "             [-0.7921977,  -4.1523376 ],\n",
    "             [ 0.9447198,  -0.46802214],\n",
    "             [ 1.158121,    3.9810789 ],\n",
    "             [ 4.768706,    2.3220146 ],\n",
    "             [ 6.1481323,   3.909829  ]])\n",
    "    minibatches = Y.batch(2)\n",
    "    for minibatch in minibatches:\n",
    "        result = target(pred, tf.transpose(minibatch))\n",
    "        break\n",
    "        \n",
    "    print(result)\n",
    "    assert(type(result) == EagerTensor), \"Use the TensorFlow API\"\n",
    "    assert (np.abs(result - (0.25361037 + 0.5566767) / 2.0) < 1e-7), \"Test does not match. Did you get the mean of your cost functions?\"\n",
    "\n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "compute_cost_test(compute_cost, new_y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "- `tf.Data.dataset = dataset.prefetch(8)` \n",
    "\n",
    "What this does is prevent a memory bottleneck that can occur when reading from disk. `prefetch()` sets aside some data and keeps it ready for when it's needed. It does this by creating a source dataset from your input data, applying a transformation to preprocess the data, then iterating over the dataset the specified number of elements at a time. This works because the iteration is streaming, so the data doesn't need to fit into the memory. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
