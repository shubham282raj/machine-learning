{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Net for Image Classification: Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#some random shit I even don't understand\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "def loadDataset():\n",
    "    trainDataset = h5py.File(\"../../Week 2/datasets/train_catvnoncat.h5\", \"r\")\n",
    "    trainDatasetX = np.array(trainDataset[\"train_set_x\"][:])\n",
    "    trainDatasetY = np.array(trainDataset[\"train_set_y\"][:])\n",
    "    \n",
    "    testDataset = h5py.File(\"../../Week 2/datasets/test_catvnoncat.h5\")\n",
    "    testDatasetX = np.array(testDataset[\"test_set_x\"][:])\n",
    "    testDatasetY = np.array(testDataset[\"test_set_y\"][:])\n",
    "\n",
    "    classes = np.array(trainDataset[\"list_classes\"][:])\n",
    "\n",
    "    trainDatasetY = trainDatasetY.reshape((1, trainDatasetY.shape[0]))\n",
    "    testDatasetY = testDatasetY.reshape((1, testDatasetY.shape[0]))\n",
    "    return trainDatasetX, trainDatasetY, testDatasetX, testDatasetY, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3) (1, 209)\n"
     ]
    }
   ],
   "source": [
    "trainDatasetX, trainDatasetY, testDatasetX, testDatasetY, classes = loadDataset()\n",
    "print(trainDatasetX.shape, trainDatasetY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of px in images: 64\n",
      "Number of training examples: 50\n"
     ]
    }
   ],
   "source": [
    "mTrain = trainDatasetX.shape[0]\n",
    "numPx = trainDatasetX.shape[1]\n",
    "mTest = testDatasetX.shape[0]\n",
    "\n",
    "print(f\"Number of training examples: {mTrain}\")\n",
    "print(f\"Number of px in images: {numPx}\")\n",
    "print(f\"Number of training examples: {mTest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainXFlatten = trainDatasetX.reshape((trainDatasetX.shape[0], -1)).T\n",
    "testXFlatten = testDatasetX.reshape((testDatasetX.shape[0], -1)).T\n",
    "\n",
    "trainX = trainXFlatten / 255\n",
    "testX = testXFlatten / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape trainX: (12288, 209)\n",
      "Shape trainDatasetY: (1, 209)\n",
      "Shape testX: (12288, 50)\n",
      "Shape testDatasetY: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape trainX: {trainX.shape}\")\n",
    "print(f\"Shape trainDatasetY: {trainDatasetY.shape}\")\n",
    "print(f\"Shape testX: {testX.shape}\")\n",
    "print(f\"Shape testDatasetY: {testDatasetY.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = trainX.shape[0]\n",
    "nh = 7\n",
    "ny = 1\n",
    "layerDims = (nx, nh, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoLayerModel(x, y, layerDims, learningRate = 0.0075, numIterations = 2500, printCost = False):\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    m = x.shape[1]\n",
    "    nx, nh, ny = layerDims\n",
    "    params = initializeParams(nx, nh, ny)\n",
    "\n",
    "    for i in range(0, numIterations):\n",
    "\n",
    "        # print(type(x), type(params[\"w1\"]), type(params[\"b1\"]))\n",
    "        # print(x.shape, params[\"w1\"].shape, params[\"b1\"].shape)\n",
    "\n",
    "        a1, cache1 = linearActivationForward(x, params[\"w1\"], params[\"b1\"], \"relu\")\n",
    "        a2, cache2 = linearActivationForward(a1, params[\"w2\"], params[\"b2\"], \"sigmoid\")\n",
    "\n",
    "        cost = computeCost(a2, y)\n",
    "\n",
    "        da2 = - (np.divide(y, a2) - np.divide(1-y, 1-a2))\n",
    "\n",
    "        da1, dw2, db2 = linearActivationBackward(da2, cache2, \"sigmoid\")\n",
    "        da0, dw1, db1, = linearActivationBackward(da1, cache1, \"relu\")\n",
    "\n",
    "        grads[\"dw1\"] = dw1\n",
    "        grads[\"db1\"] = db1\n",
    "        grads[\"dw2\"] = dw2\n",
    "        grads[\"db2\"] = db2\n",
    "\n",
    "        params = updateParams(params, grads, learningRate)\n",
    "\n",
    "        \n",
    "        if printCost and i%100 == 0: \n",
    "            costs.append(cost)\n",
    "            print(f\"Cost for {i}th iteration: {cost}\")\n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.xlabel(\"Iteration (per hundred)\")\n",
    "    plt.title(f\"Learning Rate {learningRate}\")\n",
    "    plt.show\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for 0th iteration: 0.829942437000611\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mtwoLayerModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainDatasetY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayerDims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayerDims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumIterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintCost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 22\u001b[0m, in \u001b[0;36mtwoLayerModel\u001b[1;34m(x, y, layerDims, learningRate, numIterations, printCost)\u001b[0m\n\u001b[0;32m     19\u001b[0m da2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m (np\u001b[38;5;241m.\u001b[39mdivide(y, a2) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39ma2))\n\u001b[0;32m     21\u001b[0m da1, dw2, db2 \u001b[38;5;241m=\u001b[39m linearActivationBackward(da2, cache2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m da0, dw1, db1, \u001b[38;5;241m=\u001b[39m \u001b[43mlinearActivationBackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mda1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdw1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dw1\n\u001b[0;32m     25\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m db1\n",
      "File \u001b[1;32mc:\\Users\\Shubham Raj\\Desktop\\Learning Machine Learning\\Neural Network and Deep Learning\\Week 4\\2) Deep Neural Net for Image Classification\\utils.py:191\u001b[0m, in \u001b[0;36mlinearActivationBackward\u001b[1;34m(da, cache, activation)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m activation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    189\u001b[0m     dz \u001b[38;5;241m=\u001b[39m reluBackward(da, activationCache)\n\u001b[1;32m--> 191\u001b[0m daPrev , dw, db \u001b[38;5;241m=\u001b[39m \u001b[43mlinearBackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinearCache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m daPrev, dw, db\n",
      "File \u001b[1;32mc:\\Users\\Shubham Raj\\Desktop\\Learning Machine Learning\\Neural Network and Deep Learning\\Week 4\\2) Deep Neural Net for Image Classification\\utils.py:174\u001b[0m, in \u001b[0;36mlinearBackward\u001b[1;34m(dz, cache)\u001b[0m\n\u001b[0;32m    172\u001b[0m dw \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mm \u001b[38;5;241m*\u001b[39m dz\u001b[38;5;241m.\u001b[39mdot(aPrev\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    173\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mm \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dz, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 174\u001b[0m daPrev \u001b[38;5;241m=\u001b[39m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (daPrev\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m aPrev\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (dw\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m w\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = twoLayerModel(x=trainX, y=trainDatasetY, layerDims=layerDims, numIterations=2500, printCost=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
